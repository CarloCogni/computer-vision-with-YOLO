# Segment Anything Model (SAM) Exploration

## Phase 1: Version 1 (The Baseline Assessment)

As part of the pipeline development for PPE detection, we explored Meta's Segment Anything Model (SAM) to evaluate its 
utility for automated annotation and precise pixel-level segmentation of construction site hazards.

### What Helped
* **Zero-Shot Segmentation for Distinct Objects:** SAM was highly effective at zero-shot segmenting strongly defined 
* objects like safety cones, large machinery, and high-visibility safety vests. When prompted with a rough bounding box,
* it generated highly accurate pixel masks, which could dramatically speed up the creation of future segmentation datasets.
* **Background Separation:** It excelled at separating foreground workers from complex scaffolding or dirt backgrounds, 
* provided the lighting contrast was sufficient.

### What Failed
* **Inference Speed:** SAM is significantly heavier and slower than YOLOv8n. While YOLO runs at ~1.9ms per image, 
* making it viable for live video feeds, SAM's inference time makes it impractical for real-time AECO deployment on 
* standard edge devices.
* **Ambiguous "Absence" Classes:** SAM struggles with negative classes (like `NO-Mask` or `NO-Hardhat`). Because SAM 
* segments *things*, asking it to segment the *absence* of a helmet often just resulted in segmenting a worker's hair
* or the background behind their head, adding no structural value to the detection pipeline.
* **Heavy Occlusion:** In crowded areas with overlapping workers, SAM's auto-segmentation occasionally merged multiple
* workers into a single mask or failed to isolate a hardhat partially obscured by pipes or shadows.

## Phase 2: Version 2 (The Audited Perspective)

Following the transition to a high-integrity "Clean Seed" dataset, SAM was re-evaluated specifically as a diagnostic and refinement tool for the audited 612-image set.

### What Helped
* **High-Integrity Bounding Box Refinement:** During the manual audit of 612 images, SAM's ability to generate pixel-perfect masks from rough prompts proved it is an ideal tool for correcting "label noise" in PPE bounding boxes.
* **Machinery Segmentation Consistency:** SAM’s robust performance in segmenting heavy equipment aligned with the Phase 2 success of achieving a **1.00 Vehicle Recall**, suggesting it can act as a secondary verification layer for high-risk machinery zones.
* **Data Augmentation Support:** Precise masks generated by SAM allowed for more advanced "copy-paste" augmentations, which helped maintain model performance even with a smaller, audited dataset.

### What Failed
* **Inference Gap Widening:** While the YOLOv8n V2 model achieved an optimized inference speed of **3.0ms per image** on a Tesla T4, SAM’s latency remained orders of magnitude higher, solidifying its role as an "offline" annotation tool rather than an "online" safety monitor.
* **Semantic Failure on PPE Absence:** The Phase 2 audit confirmed that SAM cannot fundamentally assist with `NO-Mask` or `NO-Hardhat` detection; its lack of semantic awareness regarding "missing" objects continues to lead to false segments of hair or skin.
* **Precision-Recall Trade-off:** While SAM provides high pixel precision, it does not improve the **26% Background-to-Person leakage** identified in the V2 Confusion Matrix, as it often segments vertical site structures (poles/scaffolding) with the same confidence as humans.
